{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes only, on the simulated dataset, file savings are commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import AlphaDropout, Conv1D, Flatten\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available(cuda_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation/split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuretype = pd.read_csv('feature_types_final_simul.csv', sep=';')\n",
    "df = pd.read_csv('data_clean_onehot_final_simulated.csv', sep=';')\n",
    "\n",
    "# preprocess\n",
    "df = df.sample(frac=1) # shuffle the data beforehand\n",
    "dataset = df.dropna().values\n",
    "\n",
    "# test and validation ratio\n",
    "test_step = .1\n",
    "\n",
    "# features and output\n",
    "X = dataset[:,0:-1]\n",
    "Y = dataset[:,-1]\n",
    "\n",
    "# indexing\n",
    "test_index = int(X.shape[0]*(1-test_step))\n",
    "\n",
    "# test split\n",
    "X_test = X[test_index:]\n",
    "Y_test = Y[test_index:]\n",
    "# Y_test = to_categorical(Y[test_index:],2)\n",
    "\n",
    "# training split\n",
    "X_train = X[:test_index]\n",
    "Y_train = Y[:test_index]\n",
    "# Y_train = to_categorical(Y[:test_index],2)\n",
    "\n",
    "\n",
    "# MinMax scaling\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)).fit(X_train)\n",
    "x_train = scaler.transform(X_train)\n",
    "x_test = scaler.transform(X_test)\n",
    "y_train = Y_train.astype(int)\n",
    "y_test = Y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the tf.keras.Sequential model by stacking layers.\n",
    "\n",
    "Defaults are based on the hyper-parameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(in_dim, dense_layer_sizes = 24, dropout_ratios = .4, depth = 7,\n",
    "                 kernin = 'glorot_uniform', opti = 'adam', activ= 'selu'):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(dense_layer_sizes,\n",
    "                    activation=activ,\n",
    "                    kernel_initializer=kernin,\n",
    "                    input_shape=(in_dim,)))\n",
    "    \n",
    "#     note: for testing Conv1D as first layer (ditched since no visible improvement)\n",
    "#     model.add(Conv1D(6, (6), activation=activ, input_shape=(32,1)))\n",
    "#     model.add(Flatten())\n",
    "\n",
    "    for i in range(depth-1): \n",
    "        model.add(Dropout(dropout_ratios))\n",
    "        # model.add(BatchNormalization())\n",
    "        # model.add(AlphaDropout(dropout_ratios))\n",
    "        model.add(Dense(dense_layer_sizes,\n",
    "                        activation=activ,\n",
    "                        kernel_initializer=kernin))\n",
    "        \n",
    "    model.add(Dense(1, activation='sigmoid',\n",
    "                    kernel_initializer=kernin)) # softmax if the output is shaped as 2-dim\n",
    "    \n",
    "    model.compile(optimizer=opti,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation training with callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=40, verbose=1)\n",
    "redlrplat = tf.keras.callbacks.ReduceLROnPlateau(patience=20, verbose=1)\n",
    "#checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='model1.hdf5', save_best_only=True, verbose=1)\n",
    "\n",
    "\n",
    "# note: for testing Conv1D as first layer (ditched since no visible improvement)\n",
    "# x_train = x_train.reshape(x_train.shape[0], x_train.shape[1],1)\n",
    "# x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1],1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "cross_histories = []\n",
    "for train_index, valid_index in skf.split(x_train, y_train):\n",
    "    model = create_model(in_dim = x_train.shape[1])\n",
    "    \n",
    "    # Train and evaluate model:\n",
    "    network_history=model.fit(x_train[train_index], y_train[train_index], \n",
    "                batch_size=16, \n",
    "                epochs=10, \n",
    "                verbose=1,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_train[valid_index], y_train[valid_index]), \n",
    "                callbacks=[early_stopping, redlrplat])\n",
    "    cross_histories.append(network_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Evaluation\")\n",
    "# model.evaluate(x_test, Y_test)\n",
    "\n",
    "def plot_history(network_histories, fp=None):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    for network_history in network_histories:\n",
    "        plt.plot(network_history.history['loss'])\n",
    "    plt.legend(['Training'])\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    for network_history in network_histories:\n",
    "        plt.plot(network_history.history['acc'])\n",
    "    plt.legend(['Training'], loc='lower right')\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    for network_history in network_histories:\n",
    "        plt.plot(network_history.history['val_loss'])\n",
    "    plt.legend(['Validation'])\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    for network_history in network_histories:\n",
    "        plt.plot(network_history.history['val_acc'])\n",
    "    plt.legend(['Validation'], loc='lower right')\n",
    "    \n",
    "#    plt.savefig(fp, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(cross_histories)#, 'model1_cross.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhh= [ h.history for h in cross_histories ]\n",
    "# pickle.dump( hhh, open( \"cv_histories_1.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=40, verbose=1)\n",
    "redlrplat = tf.keras.callbacks.ReduceLROnPlateau(patience=20, verbose=1)\n",
    "#checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='model1.hdf5', save_best_only=True, verbose=1)\n",
    "\n",
    "\n",
    "# note: for testing Conv1D as first layer (ditched since no visible improvement)\n",
    "# x_train = x_train.reshape(x_train.shape[0], x_train.shape[1],1)\n",
    "# x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1],1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "cross_histories2 = []\n",
    "for train_index, valid_index in skf.split(x_train, y_train):\n",
    "    model2 = create_model(in_dim = x_train.shape[1],\n",
    "                         dense_layer_sizes = 40, dropout_ratios = .4, depth = 6,\n",
    "                      kernin = 'glorot_normal', opti = 'rmsprop', activ= 'relu')\n",
    "    \n",
    "    # Train and evaluate model:\n",
    "    network_history2=model2.fit(x_train[train_index], y_train[train_index], \n",
    "                batch_size=16, \n",
    "                epochs=10, \n",
    "                verbose=1,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_train[valid_index], y_train[valid_index]), \n",
    "                callbacks=[early_stopping, redlrplat])\n",
    "    cross_histories2.append(network_history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(cross_histories2)#, 'model2_cross.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhh2= [ h.history for h in cross_histories2 ]\n",
    "# pickle.dump( hhh2, open( \"cv_histories_2.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=60, verbose=1)\n",
    "redlrplat = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=30, verbose=1)\n",
    "# checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='model_final.hdf5', save_best_only=True, verbose=1)\n",
    "\n",
    "\n",
    "my_classifier = KerasClassifier(create_model,\n",
    "                                in_dim = x_train.shape[1],\n",
    "                                batch_size=16, \n",
    "                                epochs=50, \n",
    "                                verbose=1,\n",
    "                                shuffle=True,\n",
    "                                validation_data=(x_test, y_test), \n",
    "                                callbacks=[early_stopping, redlrplat, checkpointer])\n",
    "\n",
    "fitted_history = my_classifier.fit(x_train, y_train)\n",
    "\n",
    "# model = create_model(in_dim = x_train.shape[1])\n",
    "# # Train and evaluate model:\n",
    "# network_history=model.fit(x_train, y_train, \n",
    "#             batch_size=16, \n",
    "#             epochs=300, \n",
    "#             verbose=1,\n",
    "#             shuffle=True,\n",
    "#             validation_data=(x_test, y_test), \n",
    "#             callbacks=[early_stopping, redlrplat, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_classifier.model.save(\"model_final2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_onemodel(network_history, fp=None):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    plt.plot(network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Test'])\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(network_history.history['acc'])\n",
    "    plt.plot(network_history.history['val_acc'])\n",
    "    plt.legend(['Training', 'Test'], loc='lower right')\n",
    "    \n",
    "#     plt.savefig(fp, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_onemodel(fitted_history)#, \"final_model.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "for comparison with the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtree = xgboost.XGBClassifier(n_estimators=100, max_depth=5)\n",
    "gbtree.fit(x_train, Y_train)\n",
    "\n",
    "print(accuracy_score(Y_train, gbtree.predict(x_train)))\n",
    "print(accuracy_score(Y_test, gbtree.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest\n",
    "for comparison with the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmod = RandomForestClassifier(n_estimators=100, max_depth= 5)\n",
    "rfmod.fit(x_train, y_train)\n",
    "\n",
    "print(rfmod.score(x_train,y_train))\n",
    "print(rfmod.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reload the NN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finmod = tf.keras.models.load_model(\"model_final2.hdf5\")\n",
    "finmod = my_classifier.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finmod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def roccc(model, modelname, tfkeras = False):\n",
    "    if tfkeras:\n",
    "        y_hat = model.predict(x_test)\n",
    "    else:\n",
    "        y_hat = model.predict_proba(x_test)[:,1]\n",
    "        \n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    fpr[0], tpr[0], _ = roc_curve(y_test, y_hat)\n",
    "    roc_auc[0] = auc(fpr[0], tpr[0])\n",
    "\n",
    "#     # Compute micro-average ROC curve and ROC area\n",
    "#     fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_hat.ravel())\n",
    "#     roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    lw = 2\n",
    "    ROC = plt.plot(fpr[0], tpr[0],\n",
    "         lw=lw, label = modelname + ' ROC (area = %0.2f)' % roc_auc[0])\n",
    "    \n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "roccc(finmod, tfkeras=True, modelname = 'DNN')\n",
    "roccc(gbtree, modelname = 'XGBoost')\n",
    "roccc(rfmod, modelname = 'RandFor')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# plt.savefig('roc_compa.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_sklearn_model():\n",
    "#     dens = np.random.choice([16,24,32,40])\n",
    "#     dep = np.random.choice([3,4,5,6,7])\n",
    "#     act = np.random.choice(['selu', 'relu', 'sigmoid'])\n",
    "#     drop =np.random.choice([.2,.3,.4,.5])\n",
    "    \n",
    "    my_classifier = KerasClassifier(create_model,\n",
    "                                    in_dim=x_train.shape[1],\n",
    "                                    dense_layer_sizes = 24,\n",
    "                                    dropout_ratios = .4,\n",
    "                                    depth = 7,\n",
    "                                    activ= 'selu',\n",
    "                                batch_size=16, \n",
    "                                epochs=50, \n",
    "                                verbose=1,\n",
    "                                shuffle=True)\n",
    "    return my_classifier\n",
    "\n",
    "\n",
    "bagclf = BaggingClassifier(base_estimator=create_sklearn_model(),\n",
    "                           n_estimators=10,\n",
    "                           random_state=0,\n",
    "                          bootstrap=True)\n",
    "\n",
    "bagclf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagclf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "bagclf_disp = plot_roc_curve(bagclf, x_train, y_train)\n",
    "bagclf_disp.figure_.suptitle(\"ROC curve comparison\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
